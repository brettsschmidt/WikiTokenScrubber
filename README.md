# WikiTokenScrubber
Data cleaning routine for converting the Wiki2 Token Scrubber into matrices for machine learning

Indexes the words in the Wikitext Long Term Dependency Dataset.

Produces 5x5 matrices of the sentences with each element being the word's index and turns them into a space-delimited txt file with each line being a matrix. Also produces the index into a .txt file.

The Datasets can be found at https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/
